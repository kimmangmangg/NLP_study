## 09-01. 워드 임베딩(Word Embedding)<br>

- 워드 임베딩은 단어를 밀집 표현(Dense Representation)으로 변환하는 방법임.
- 단어 의미적 유사성을 반영할 수 있는 벡터 표현 방법임.

### 1) 희소 표현 (Sparse Representation)

- 원-핫 인코딩은 단어 집합 크기만큼 차원을 가지며 해당 단어 인덱스만 1, 나머지는 0으로 표현함
- 차원이 단어 집합 크기와 동일 → 고차원
- 대부분 값이 0인 희소 벡터
- **단어 간 의미적 유사성을 반영하지 못함**

```python
강아지 = [0, 0, 0, 0, 1, 0, 0, ..., 0]   (차원: 10,000)
```

**[문제점]**
- 공간 낭비 (고차원 희소 행렬)
- 의미 반영 불가 (단어 간 관계 고려 없음)
- DTM(문서-단어 행렬)도 희소 표현의 대표적 사례


### 2) 밀집 표현 (Dense Representation)

- 희소 표현과 달리 차원을 작게 설정하고, 값은 실수로 구성됨
- 모든 단어가 동일한 크기의 벡터로 매핑됨 (보통 50~300차원)
- **의미적 정보를 반영 가능**

```python
강아지 = [0.2, 1.8, 1.1, -2.1, 1.1, 2.8, ...]   (차원: 128)
```

### 3) 워드 임베딩 (Word Embedding)

- 단어를 밀집 벡터로 표현하는 기법
- 학습을 통해 얻어진 벡터를 임베딩 벡터(embedding vector)라 함
- 대표적인 방법론: LSA, Word2Vec, FastText, GloVe
- 케라스 `Embedding()`

  - 초기엔 무작위 값으로 임베딩 벡터 생성
  - 신경망 학습 과정에서 가중치처럼 업데이트되어 단어 벡터 학습

### 📌 원-핫 벡터 vs 임베딩 벡터

| 구분    | 원-핫 벡터         | 임베딩 벡터          |
| ----- | -------------- | --------------- |
| 차원    | 고차원 (단어 집합 크기) | 저차원 (보통 50~300) |
| 종류    | 희소 벡터          | 밀집 벡터           |
| 표현 방법 | 수동 (인덱스 기반)    | 학습 기반           |
| 값의 타입 | 0과 1           | 실수              |


### 📌 핵심 정리

- 원-핫 인코딩은 희소 벡터로 의미 반영 불가
- 밀집 표현은 저차원 실수 벡터로 의미 반영 가능
- 워드 임베딩은 단어를 밀집 벡터로 표현하는 표준 기법
- 임베딩 벡터는 학습을 통해 단어 간 관계를 반영
<br><br>
---
<br><br><br>

