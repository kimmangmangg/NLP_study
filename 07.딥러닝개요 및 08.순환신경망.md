## 07-01. 퍼셉트론 (Perceptron)<br>

* 퍼셉트론은 가장 기본적인 인공 신경망의 형태로, **단층 신경망(single-layer neural network)** 임.
* 입력에 가중치를 곱한 뒤, 합산한 결과를 활성화 함수(Activation Function)에 통과시켜 출력함.

### 1) 구조

* 입력층(Input) → 가중치(Weight) → 합산 → 활성화 함수 → 출력(Output)
<img width="690" height="173" alt="Image" src="https://github.com/user-attachments/assets/faf2b37d-38c0-4c2a-856f-f97a7e2fc8c5" />

### 2) 특징

* 선형 분류기(Linear Classifier)로 동작
* 특정 임계값(threshold)을 기준으로 0 또는 1 출력

### 3) 한계

* XOR 문제와 같이 **비선형 분류 문제 해결 불가**
* 복잡한 패턴 학습에 제약이 있음

### 4) 의의

* 신경망 이론의 기초 개념 제공
* 다층 퍼셉트론(MLP, Multi-Layer Perceptron)으로 확장되며 딥러닝 발전의 초석이 됨

### 📌 핵심 정리

* 퍼셉트론은 가장 단순한 형태의 인공 신경망
* 입력·가중치·편향·활성화 함수로 구성
* 선형 문제는 해결 가능하나 비선형 문제(XOR 등)는 해결 불가
* 이후 다층 구조(MLP)로 확장되어 딥러닝 발전에 기여

<br><br>

## 07-02. 인공 신경망(Artificial Neural Network, ANN) 훑어보기<br>

* 인공 신경망(ANN)은 인간의 뇌 신경망(Neuron Network)을 모방한 기계 학습 모델임.
* 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 구성되며, 다층 구조로 확장되어 복잡한 패턴 학습 가능.

### 1) 기본 구조

* **입력층(Input Layer)**: 입력 데이터 수용
* **은닉층(Hidden Layer)**: 가중치·편향·활성화 함수로 비선형 변환 수행
* **출력층(Output Layer)**: 최종 예측값 출력

### 2) 동작 원리

1. 입력 데이터 (x)에 가중치 (w)와 편향 (b) 적용
2. 합산 후 활성화 함수(Activation Function)에 통과
3. 출력 값 생성 → 다음 층 입력으로 전달
4. 출력층에서 손실 함수(Loss Function) 계산
5. 역전파(Backpropagation) 알고리즘으로 가중치 갱신

### 3) 활성화 함수의 역할

* 비선형성을 부여하여 복잡한 문제 학습 가능하게 함
* 예시: 시그모이드, ReLU, 하이퍼볼릭 탄젠트 등

### 4) ANN의 특징

* 단층 퍼셉트론 → 선형 문제만 해결
* 다층 퍼셉트론(MLP) → 비선형 문제(XOR 등) 해결 가능
* 딥러닝(Deep Learning)은 다층 ANN의 심화된 형태

### 📌 핵심 정리

* ANN은 입력층, 은닉층, 출력층으로 구성된 신경망 모델
* 가중치, 편향, 활성화 함수가 핵심 요소
* 역전파 알고리즘으로 학습 수행
* 다층 구조를 통해 비선형 문제 해결 가능, 딥러닝의 기반이 됨

<br><br>

## 07-03. 행렬곱으로 이해하는 신경망<br>

* 신경망은 입력과 가중치의 **행렬곱(matrix multiplication)** 연산으로 표현 가능함.
* 여러 입력을 동시에 처리하고 병렬 연산을 수행하기 위해 행렬 연산을 활용함.

### 1) 퍼셉트론 수식의 행렬 표현

* 단일 뉴런:
<img width="710" height="38" alt="Image" src="https://github.com/user-attachments/assets/52df55ee-631d-4761-a703-c376f4f16054" />
* 행렬곱으로 확장:
<img width="664" height="137" alt="Image" src="https://github.com/user-attachments/assets/e07a3f65-84fc-4c8d-ab47-02f7f1582354" />

### 2) 벡터와 행렬 연산

* 여러 입력 벡터를 한 번에 처리 가능
* 신경망의 forward propagation 과정은 행렬곱과 활성화 함수 적용의 연속

### 3) 장점

* 계산 효율성 증가 (병렬 연산 가능)
* GPU 활용 시 대규모 연산을 빠르게 수행 가능

### 📌 핵심 정리

* 신경망 연산은 **행렬곱 + 활성화 함수**의 반복으로 표현됨
* 행렬곱은 병렬 연산을 가능하게 하여 딥러닝 학습 효율성을 높임
* 신경망을 이해하는 데 행렬곱 관점이 필수적임

<br><br>

## 07-04. 딥 러닝의 학습 방법<br>

* 딥 러닝 모델은 **순전파(Forward Propagation)** 와 **역전파(Backpropagation)** 과정으로 학습함.
* 목표는 손실 함수(Loss)를 최소화하도록 가중치와 편향을 최적화하는 것임.

### 1) 순전파 (Forward Propagation)

* 입력 데이터를 신경망에 통과시켜 출력 계산
* 예측 결과와 실제 값 비교 → 손실 함수로 오차 계산

### 2) 역전파 (Backpropagation)

* 오차를 출력층에서 입력층 방향으로 전파
* 각 가중치에 대한 기울기(Gradient) 계산
* 경사하강법(Gradient Descent)으로 가중치 업데이트

### 3) 최적화 과정

* 손실 함수를 최소화하는 방향으로 파라미터 조정
* 옵티마이저: SGD, Adam, RMSprop 등

### 4) 핵심 요소

* **손실 함수**: 모델 예측 성능 평가 (예: MSE, Cross-Entropy)
* **학습률(Learning Rate)**: 가중치 업데이트 보폭
* **에폭(Epoch)**: 전체 데이터셋을 학습한 횟수

### 📌 핵심 정리

* 딥 러닝 학습 = 순전파 + 역전파 반복
* 손실 함수와 옵티마이저를 통해 가중치 최적화
* 하이퍼파라미터(학습률, 에폭 등)가 학습 성능에 큰 영향

<br><br>
