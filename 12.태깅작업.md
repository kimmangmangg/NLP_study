## 12-01. 케라스를 이용한 태깅 작업 개요<br>

* 태깅 작업(Tagging Task)은 **문장의 각 단어에 레이블을 부여**하는 문제임.
* 대표적인 태깅 작업으로 품사 태깅, 개체명 인식(NER) 등이 있음.

### 1) 태깅 작업의 특징

* 입력: 단어 시퀀스(문장)
* 출력: 단어별 태그 시퀀스
* 지도 학습(Supervised Learning) 문제로 접근 가능

### 2) 주요 태깅 작업

* **품사 태깅 (POS Tagging)**: 단어별 품사(명사, 동사 등) 부여
* **개체명 인식 (Named Entity Recognition, NER)**: 인물, 지명, 날짜 등 특정 의미 범주 태깅

### 3) 모델 구조

* 임베딩 레이어로 단어를 벡터화
* RNN, LSTM, GRU 등 순환 신경망으로 시퀀스 처리
* 최종 출력층에서 단어별 태그 확률 분포 산출 (Softmax)<br>
<img width="503" height="575" alt="Image" src="https://github.com/user-attachments/assets/94fc6d13-7ae2-4c35-95fc-8e8e17cfa55e" /><br>


### 4) 응용 분야

* 형태소 분석, 개체명 추출, 의미역 결정 등
* 정보 추출, 질의응답, 챗봇 등 다양한 NLP 태스크에 활용

### 📌 핵심 정리

* 태깅 작업은 단어별 레이블 예측 문제
* 품사 태깅, 개체명 인식이 대표적 예시
* RNN 계열 모델과 임베딩을 조합하여 효과적으로 처리 가능

<br><br>

## 12-02. 양방향 LSTM을 이용한 품사 태깅<br>

* 품사 태깅(POS tagging)은 문장의 각 단어에 품사를 부여하는 작업임.
* RNN/LSTM은 순차적 입력에 강점이 있으나, 과거 방향만 고려 → 미래 단어 정보 부족.
* **Bi-LSTM**은 순방향과 역방향 LSTM을 결합하여 문맥을 더 풍부하게 반영함.

### 1) Bi-LSTM 구조

* **순방향 LSTM**: 앞에서 뒤로 입력 처리
* **역방향 LSTM**: 뒤에서 앞으로 입력 처리
* 두 출력을 연결(concatenate)하여 단어별 최종 벡터 생성

### 2) 모델 구현 절차

1. 임베딩 레이어: 단어 인덱스를 벡터로 변환
2. Bi-LSTM 레이어: 순방향/역방향 LSTM 결합
3. TimeDistributed(Dense): 각 시점별로 태그 예측
4. Softmax 출력층: 태그 확률 분포 산출

### 3) Keras 코드 예시

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
model.add(TimeDistributed(Dense(num_tags, activation="softmax")))

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()
```

### 4) 장점

* 단어의 앞뒤 문맥을 동시에 고려 → 품사 예측 정확도 향상
* 다의어 문제 해결에 도움 ("watch" → 동사/명사 구분 등)

### 📌 핵심 정리

* Bi-LSTM은 순방향+역방향 LSTM 결합 구조
* 문맥 의존적 정보(앞뒤 단어)를 모두 활용해 품사 태깅 성능 개선
* 태깅, 개체명 인식 등 시퀀스 레이블링 문제에 효과적

<br><br>


## 12-03. 개체명 인식 (Named Entity Recognition, NER)<br>

* 개체명 인식은 문장에서 특정 의미를 가진 단어(개체명)에 태그를 부여하는 작업임.
* 예: 사람 이름, 지명, 기관명, 날짜, 시간 등을 식별하여 레이블링.

### 1) 개체명 인식의 정의

* 입력: 단어 시퀀스(문장)
* 출력: 각 단어에 대해 개체명 태그 부여
* 정보 추출(Information Extraction)의 핵심 과정

### 2) 대표적인 개체명 태그

* PER (인명, Person)
* LOC (지명, Location)
* ORG (기관명, Organization)
* DATE (날짜)
* TIME (시간)
* MISC (기타 고유명사)

### 3) 활용 사례

* **검색 시스템**: 인명·지명 검색 정확도 향상
* **질의응답 시스템**: “스티브 잡스는 누구야?” → PER 인식
* **챗봇/대화 시스템**: 사용자 입력에서 인명, 장소 추출
* **정보 추출/요약**: 문서에서 핵심 개체 추출

### 4) 모델링 접근

* **기계 학습 기반**: CRF, HMM 등 전통적 시퀀스 모델
* **딥러닝 기반**: Bi-LSTM, CNN, Transformer 기반 모델
* **Bi-LSTM + CRF** 구조가 널리 사용됨

### 5) 태깅 스킴(Tagging Scheme)

* BIO 방식 (Begin, Inside, Outside)

  * B-PER: 인명 시작 / I-PER: 인명 내부 / O: 개체명 아님
* BIOES, IOB2 등 변형도 존재

### 📌 핵심 정리

* NER은 문장에서 인명, 지명, 기관명 등 **개체명 태깅** 작업
* 정보 추출, QA, 챗봇 등 다양한 NLP 응용에 활용
* BIO 태깅 스킴과 Bi-LSTM+CRF 등의 모델 구조가 대표적 접근 방식

<br><br>

## 12-04. 개체명 인식의 BIO 표현 이해하기<br>

* 개체명 인식(NER)에서 단어의 시작과 내부를 구분하기 위해 BIO 태깅 방식을 사용함.
* BIO 태그는 **B (Begin), I (Inside), O (Outside)** 로 구성됨.

### 1) BIO 태그 구조

* **B-XXX** : 개체명이 시작되는 단어 (예: B-PER, B-LOC)
* **I-XXX** : 개체명 내부 단어
* **O** : 개체명이 아닌 일반 단어

### 2) 예시

문장: `Steve Jobs founded Apple`

| 단어      | 태그    |
| ------- | ----- |
| Steve   | B-PER |
| Jobs    | I-PER |
| founded | O     |
| Apple   | B-ORG |

### 3) 장점

* 개체명 경계(boundary)를 명확히 구분 가능
* 다단어 개체명(예: "New York City")도 정확히 태깅 가능

### 4) 변형 스킴

* **IOB2**: 모든 개체명의 첫 단어를 B로 표시
* **BIOES**: Begin, Inside, Outside, End, Single

  * 개체명 끝(End)과 단일 단어(Single) 개체명까지 구분 가능

### 📌 핵심 정리

* BIO 태그는 NER에서 개체명 시작·내부·외부를 구분하는 방식
* 다단어 개체명 표현에 효과적
* IOB2, BIOES 등 다양한 변형 존재

<br><br>

## 12-05. BiLSTM을 이용한 개체명 인식 (NER)<br>

* 개체명 인식(NER)은 문장의 단어마다 PER, LOC, ORG 등 개체명 태그를 부여하는 작업임.
* BiLSTM은 양방향 문맥을 활용해 단어 의미를 풍부하게 반영하므로 NER에 적합함.

### 1) BiLSTM 기반 NER 구조

1. **임베딩 레이어**: 단어 인덱스를 임베딩 벡터로 변환
2. **BiLSTM 레이어**: 앞뒤 문맥을 동시에 고려해 시퀀스 정보 학습
3. **Dense 레이어 + Softmax**: 각 단어에 대해 태그 확률 분포 산출

### 2) Keras 모델 예시

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, TimeDistributed, Dense

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))
model.add(Bidirectional(LSTM(units=64, return_sequences=True)))
model.add(TimeDistributed(Dense(num_tags, activation="softmax")))

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.summary()
```

* `return_sequences=True`: 각 시점마다 출력 필요
* `TimeDistributed(Dense(...))`: 단어별 태그 예측

### 3) 장점

* 앞뒤 문맥 정보 반영 → 정확한 개체명 판별 가능
* 다의어 처리, 긴 문맥 의존 관계 파악에 강점

### 4) 한계와 보완

* BiLSTM 단독으로는 태그 간 제약조건 고려 부족
* CRF(Conditional Random Field)와 결합 시 더 나은 성능

### 📌 핵심 정리

* BiLSTM은 순방향+역방향 문맥을 모두 활용하는 RNN 구조
* 개체명 인식에 효과적이며, CRF와 함께 쓰면 성능 향상
* NER, 품사 태깅 등 시퀀스 레이블링 문제에 널리 사용됨

<br><br>

## 12-06. BiLSTM-CRF를 이용한 개체명 인식<br>

* NER에서 BiLSTM은 문맥 정보 반영에 강점이 있으나, 태그 간 구조적 제약 반영 부족.
* 이를 보완하기 위해 **CRF(Conditional Random Field)** 레이어를 출력층에 추가함.

### 1) BiLSTM-CRF 구조

* 임베딩 → BiLSTM → CRF
* BiLSTM: 단어의 양방향 문맥 정보 반영
* CRF: 연속된 태그 간 규칙성(예: B-PER 뒤에는 I-PER 가능)을 고려<br>
<img width="498" height="377" alt="Image" src="https://github.com/user-attachments/assets/37176eee-da20-4f28-8ae8-c93d8702a3cc" /><br>

### 2) 장점

* 단어 단위 독립 예측의 한계 극복
* 태그 시퀀스 전체의 전역적 일관성 보장
* NER, 품사 태깅 등에서 최신 성능 달성

### 3) Keras 구현 예시

```python
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, TimeDistributed, Dense
from tensorflow_addons.layers import CRF

input_layer = Input(shape=(max_len,))
model = Embedding(input_dim=vocab_size, output_dim=128)(input_layer)
model = Bidirectional(LSTM(units=64, return_sequences=True))(model)
crf = CRF(num_tags)
output = crf(model)

ner_model = Model(input_layer, output)
ner_model.compile(optimizer="adam", loss=crf.loss, metrics=[crf.accuracy])
```

### 4) 적용 사례

* 개체명 인식 (인물, 지명, 기관명 태깅)
* 형태소 분석, 품사 태깅, 의미역 결정

### 📌 핵심 정리

* BiLSTM은 문맥 정보, CRF는 전역 태그 제약을 반영
* 결합 구조(BiLSTM-CRF)는 NER에서 매우 효과적
* 현재도 널리 쓰이는 강력한 시퀀스 레이블링 모델

<br><br>

## 12-07. 문자 임베딩(Character Embedding) 활용하기<br>

* 문자 임베딩은 **단어를 문자 단위로 분해하여 임베딩**하는 기법임.
* 한국어·중국어처럼 **형태소 구조가 복잡하거나 희귀 단어가 많은 언어**에서 효과적임.

### 1) 필요성

* 단어 단위 임베딩(Word2Vec 등)은 **OOV 문제(Out-Of-Vocabulary)** 발생
* 신조어, 오타, 복합어 처리에 한계
* 문자 임베딩은 단어를 문자 시퀀스로 보고 학습 → OOV 완화

### 2) 방법

* 입력 단어 → 문자 단위 토큰화 → 임베딩 레이어 → CNN/RNN 기반 인코딩
* 문자 임베딩 결과를 단어 임베딩과 결합하여 문맥 표현 강화

### 3) 장점

* OOV 문제 완화
* 형태소 단위 정보 반영 가능
* 단어 내부 구조(접두사, 접미사) 활용

### 4) 활용 예시

* **FastText**: subword(부분 문자열) 기반 단어 임베딩
* **한국어·중국어 처리**: 복잡한 형태 변화 단어에도 강건

### 📌 핵심 정리

* 문자 임베딩은 단어를 문자 단위로 벡터화하는 기법
* 희귀어·신조어·오타 처리에서 강점
* Word Embedding과 함께 사용 시 NLP 모델 성능 개선
