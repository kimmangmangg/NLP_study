## 07-01. 퍼셉트론 (Perceptron)<br>

* 퍼셉트론은 가장 기본적인 인공 신경망의 형태로, **단층 신경망(single-layer neural network)** 임.
* 입력에 가중치를 곱한 뒤, 합산한 결과를 활성화 함수(Activation Function)에 통과시켜 출력함.

### 1) 구조

* 입력층(Input) → 가중치(Weight) → 합산 → 활성화 함수 → 출력(Output)
<img width="690" height="173" alt="Image" src="https://github.com/user-attachments/assets/faf2b37d-38c0-4c2a-856f-f97a7e2fc8c5" />

### 2) 특징

* 선형 분류기(Linear Classifier)로 동작
* 특정 임계값(threshold)을 기준으로 0 또는 1 출력

### 3) 한계

* XOR 문제와 같이 **비선형 분류 문제 해결 불가**
* 복잡한 패턴 학습에 제약이 있음

### 4) 의의

* 신경망 이론의 기초 개념 제공
* 다층 퍼셉트론(MLP, Multi-Layer Perceptron)으로 확장되며 딥러닝 발전의 초석이 됨

### 📌 핵심 정리

* 퍼셉트론은 가장 단순한 형태의 인공 신경망
* 입력·가중치·편향·활성화 함수로 구성
* 선형 문제는 해결 가능하나 비선형 문제(XOR 등)는 해결 불가
* 이후 다층 구조(MLP)로 확장되어 딥러닝 발전에 기여

<br><br>

## 07-02. 인공 신경망(Artificial Neural Network, ANN) 훑어보기<br>

* 인공 신경망(ANN)은 인간의 뇌 신경망(Neuron Network)을 모방한 기계 학습 모델임.
* 입력층(Input layer), 은닉층(Hidden layer), 출력층(Output layer)으로 구성되며, 다층 구조로 확장되어 복잡한 패턴 학습 가능.

### 1) 기본 구조

* **입력층(Input Layer)**: 입력 데이터 수용
* **은닉층(Hidden Layer)**: 가중치·편향·활성화 함수로 비선형 변환 수행
* **출력층(Output Layer)**: 최종 예측값 출력

### 2) 동작 원리

1. 입력 데이터 (x)에 가중치 (w)와 편향 (b) 적용
2. 합산 후 활성화 함수(Activation Function)에 통과
3. 출력 값 생성 → 다음 층 입력으로 전달
4. 출력층에서 손실 함수(Loss Function) 계산
5. 역전파(Backpropagation) 알고리즘으로 가중치 갱신

### 3) 활성화 함수의 역할

* 비선형성을 부여하여 복잡한 문제 학습 가능하게 함
* 예시: 시그모이드, ReLU, 하이퍼볼릭 탄젠트 등

### 4) ANN의 특징

* 단층 퍼셉트론 → 선형 문제만 해결
* 다층 퍼셉트론(MLP) → 비선형 문제(XOR 등) 해결 가능
* 딥러닝(Deep Learning)은 다층 ANN의 심화된 형태

### 📌 핵심 정리

* ANN은 입력층, 은닉층, 출력층으로 구성된 신경망 모델
* 가중치, 편향, 활성화 함수가 핵심 요소
* 역전파 알고리즘으로 학습 수행
* 다층 구조를 통해 비선형 문제 해결 가능, 딥러닝의 기반이 됨

<br><br>

## 07-03. 행렬곱으로 이해하는 신경망<br>

* 신경망은 입력과 가중치의 **행렬곱(matrix multiplication)** 연산으로 표현 가능함.
* 여러 입력을 동시에 처리하고 병렬 연산을 수행하기 위해 행렬 연산을 활용함.

### 1) 퍼셉트론 수식의 행렬 표현

* 단일 뉴런:<br>
<img width="710" height="38" alt="Image" src="https://github.com/user-attachments/assets/52df55ee-631d-4761-a703-c376f4f16054" /><br>
* 행렬곱으로 확장:<br>
<img width="664" height="137" alt="Image" src="https://github.com/user-attachments/assets/e07a3f65-84fc-4c8d-ab47-02f7f1582354" /><br>

### 2) 벡터와 행렬 연산

* 여러 입력 벡터를 한 번에 처리 가능
* 신경망의 forward propagation 과정은 행렬곱과 활성화 함수 적용의 연속

### 3) 장점

* 계산 효율성 증가 (병렬 연산 가능)
* GPU 활용 시 대규모 연산을 빠르게 수행 가능

### 📌 핵심 정리

* 신경망 연산은 **행렬곱 + 활성화 함수**의 반복으로 표현됨
* 행렬곱은 병렬 연산을 가능하게 하여 딥러닝 학습 효율성을 높임
* 신경망을 이해하는 데 행렬곱 관점이 필수적임

<br><br>

## 07-04. 딥 러닝의 학습 방법<br>

* 딥 러닝 모델은 **순전파(Forward Propagation)** 와 **역전파(Backpropagation)** 과정으로 학습함.
* 목표는 손실 함수(Loss)를 최소화하도록 가중치와 편향을 최적화하는 것임.

### 1) 순전파 (Forward Propagation)

* 입력 데이터를 신경망에 통과시켜 출력 계산
* 예측 결과와 실제 값 비교 → 손실 함수로 오차 계산

### 2) 역전파 (Backpropagation)

* 오차를 출력층에서 입력층 방향으로 전파
* 각 가중치에 대한 기울기(Gradient) 계산
* 경사하강법(Gradient Descent)으로 가중치 업데이트

### 3) 최적화 과정

* 손실 함수를 최소화하는 방향으로 파라미터 조정
* 옵티마이저: SGD, Adam, RMSprop 등

### 4) 핵심 요소

* **손실 함수**: 모델 예측 성능 평가 (예: MSE, Cross-Entropy)
* **학습률(Learning Rate)**: 가중치 업데이트 보폭
* **에폭(Epoch)**: 전체 데이터셋을 학습한 횟수

### 📌 핵심 정리

* 딥 러닝 학습 = 순전파 + 역전파 반복
* 손실 함수와 옵티마이저를 통해 가중치 최적화
* 하이퍼파라미터(학습률, 에폭 등)가 학습 성능에 큰 영향

<br><br>

## 07-05. 역전파(BackPropagation) 이해하기<br>

* 역전파는 신경망 학습의 핵심 알고리즘으로, **출력층의 오차를 입력층까지 거슬러 올라가며 가중치의 기울기를 계산**하는 방법임.
* 목표: 손실 함수의 값을 최소화하기 위해 가중치와 편향을 최적화.

### 1) 순전파 (Forward Propagation)

* 입력 데이터를 통해 출력을 계산.
* 출력과 실제 값 비교 → 손실 함수로 오차 계산.

### 2) 역전파 과정

* 출력층부터 시작해 은닉층, 입력층 방향으로 오차 전파.
* 각 층의 가중치에 대해 미분(기울기) 계산.
* 연쇄 법칙(Chain Rule)을 사용하여 기울기 효율적 계산.

### 3) 가중치 갱신

* 계산된 기울기를 이용해 경사하강법으로 가중치 업데이트.
* [
  w := w - \eta \frac{\partial L}{\partial w}
  ]

  * (\eta): 학습률(Learning rate)

### 4) 특징

* 신경망 학습을 가능하게 한 핵심 알고리즘.
* 층이 깊어질수록 **기울기 소실/폭주 문제** 발생 가능.
* 이를 해결하기 위해 ReLU, LSTM, Batch Normalization 등 다양한 기법 사용.

### 📌 핵심 정리

* 역전파 = 오차 역전파 알고리즘.
* 출력층의 오차를 입력층으로 전파해 가중치의 기울기 계산.
* 경사하강법을 통해 손실 함수 최소화.
* 딥러닝 학습의 기본 원리.

<br><br>

## 07-06. 과적합(Overfitting)을 막는 방법들<br>

* 과적합은 모델이 학습 데이터에 지나치게 적합하여, 새로운 데이터에 대한 일반화 성능이 떨어지는 현상임.
* 딥러닝에서 자주 발생하며, 이를 방지하기 위한 다양한 기법이 존재함.

### 1) 데이터 관련 기법

* **데이터 양 늘리기**: 더 많은 학습 데이터 확보
* **데이터 증강(Data Augmentation)**: 기존 데이터를 변형·추가

### 2) 모델 단순화

* 파라미터 수 줄이기 (층 수, 뉴런 수 감소)
* 불필요하게 복잡한 네트워크 구조 피하기

### 3) 정규화 기법

* **L1/L2 정규화**: 가중치 크기를 제한하여 과적합 방지
* **드롭아웃(Dropout)**: 학습 시 일부 뉴런을 확률적으로 제외 → 모델 일반화 성능 향상

### 4) 학습 기법

* **조기 종료(Early Stopping)**: 검증 데이터 성능이 악화되면 학습 중단
* **배치 정규화(Batch Normalization)**: 각 층의 입력 분포를 정규화 → 안정적 학습

### 5) 앙상블

* 여러 모델의 예측을 결합하여 성능 향상 및 과적합 완화

### 📌 핵심 정리

* 과적합은 학습 데이터에는 강하지만, 새로운 데이터에 취약한 현상
* 데이터 증강, 모델 단순화, 정규화, 드롭아웃, 조기 종료 등이 주요 해결책
* 일반화 성능 향상을 위해 다양한 방법을 병행하는 것이 효과적

<br><br>
## 07-07. 기울기 소실(Gradient Vanishing)과 폭주(Exploding)<br>

* 역전파 과정에서 기울기(Gradient)가 0에 가까워지거나, 반대로 너무 커지는 현상 발생 가능.
* 이는 학습의 불안정성 또는 학습 불가능 문제를 초래함.

### 1) 기울기 소실 (Gradient Vanishing)

* 활성화 함수(sigmoid, tanh 등) 사용 시 미분 값이 작아져, 역전파 시 곱해지며 기울기가 0에 수렴.
* 깊은 신경망에서 초기 층의 가중치가 거의 갱신되지 않아 학습 불가.

### 2) 기울기 폭주 (Gradient Exploding)

* 역전파 시 기울기가 계속 곱해지며 값이 기하급수적으로 커짐.
* 가중치가 비정상적으로 커지고 학습 발산.

### 3) 해결 방법

* **가중치 초기화**: Xavier, He 초기화 등 안정적 분포 사용
* **ReLU 계열 활성화 함수**: sigmoid, tanh보다 기울기 소실 완화
* **Gradient Clipping**: 기울기의 최대 크기를 제한하여 폭주 방지
* **정규화 기법**: Batch Normalization으로 학습 안정화

### 📌 핵심 정리

* 기울기 소실: 기울기 → 0, 학습 멈춤
* 기울기 폭주: 기울기 → ∞, 학습 발산
* 초기화, ReLU, Gradient Clipping, BatchNorm 등으로 문제 완화

<br><br>
## 07-11. 다층 퍼셉트론(MultiLayer Perceptron, MLP)으로 텍스트 분류하기<br>

* 다층 퍼셉트론(MLP)은 입력층-은닉층-출력층으로 구성된 기본 신경망 구조임.
* 텍스트 분류 작업에서도 BoW, TF-IDF 등 전처리된 벡터를 입력으로 활용해 분류 가능함.

### 1) 입력 표현

* 텍스트 → 정수 인코딩, 원-핫 벡터, TF-IDF 등으로 변환
* 입력 차원 = 단어 집합 크기

### 2) 모델 구조

* 입력층: 텍스트 벡터 입력
* 은닉층: Dense Layer + 활성화 함수(ReLU 등)
* 출력층: Softmax로 다중 클래스 분류 수행

### 3) 학습 과정

* 손실 함수: Cross Entropy Loss
* 옵티마이저: Adam, SGD 등
* Epoch 반복 학습으로 가중치 업데이트

### 4) 예시 코드

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(vocab_size,)))
model.add(Dense(64, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

### 5) 특징

* 단순하지만 강력한 분류 모델
* 입력 표현(BoW, TF-IDF 등)에 따라 성능 좌우됨
* 문맥 정보 부족 → RNN, CNN, Transformer 기반 모델로 확장

### 📌 핵심 정리

* MLP는 기본적인 텍스트 분류 신경망 구조
* 입력층-은닉층-출력층으로 구성, Softmax로 다중 분류 수행
* BoW, TF-IDF 같은 단어 기반 벡터 표현 사용
* 심층 구조로 확장 가능하나, 문맥 처리 한계 존재

<br><br>

## 07-12. 피드 포워드 신경망 언어 모델 (NNLM)<br>

* NNLM(Neural Network Language Model)은 확률적 언어모델을 신경망 기반으로 구현한 것임.
* 전통적인 N-gram 모델의 한계(차원의 저주, 희소성 문제)를 극복하기 위해 제안됨.

### 1) 구조

* 입력: 단어 시퀀스(앞 단어들)
* 임베딩 레이어: 단어를 밀집 벡터로 변환
* 은닉층: 비선형 변환 (예: ReLU, tanh)
* 출력층: Softmax → 다음 단어 확률 분포 예측

### 2) 특징

* 단어를 밀집 벡터로 학습하여 일반화 성능 향상
* N-gram 기반보다 적은 파라미터로 표현 가능
* 단어 의미 공간을 학습하면서 문맥 기반 예측 가능

### 3) 학습 과정

* 입력 단어 시퀀스로 다음 단어 예측
* 손실 함수: Cross Entropy Loss
* 경사하강법 + 역전파로 가중치 최적화

### 4) 장점과 한계

* **장점**: 차원의 저주 완화, 단어 의미 반영 가능
* **한계**: 고정된 윈도우 크기 내에서만 문맥 고려 (장기 의존성 부족)

### 📌 핵심 정리

* NNLM은 신경망을 활용한 언어 모델로, N-gram의 한계를 보완
* 단어 임베딩을 학습하면서 문맥 기반 다음 단어 확률 예측
* RNN 기반 언어 모델(RNNLM)로 발전하는 토대가 됨

<br><br>

---
